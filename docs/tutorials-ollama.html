<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tutorials - Moya Documentation</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <link rel="icon" href="https://montycloud.com/hubfs/icon-for-favicon-1.png" type="image/png">
</head>

<body>
    <header>
        <h1>Moya Documentation</h1>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="quickstart.html">Quickstart</a></li>
                <li><a href="guides.html">Guides</a></li>
                <li><a href="explanations.html">Explanations</a></li>
                <li><a href="tutorials.html">Tutorials</a></li>
                <li><a href="reference.html">Reference</a></li>
            </ul>
        </nav>
    </header>

    <div class="container">
        <aside class="sidebar">
            <h3>Tutorials</h3>
            <ul>
                <li><a href="tutorials-simple.html">Simple Agent</a></li>
                <li><a href="tutorials-multi.html">Multi-Agent System</a></li>
                <!-- <li><a href="tutorials-dynamic.html">Dynamic Agents</a></li> -->
                <li><a href="tutorials-remote.html">Remote Agent</a></li>
                <li><a href="tutorials-ollama.html">Ollama Integration</a></li>
            </ul>
        </aside>

        <main>
            <h2>Tutorials</h2>

            <h3 id="ollama-integration">Integrating with Ollama for Local LLMs</h3>
            <p>In this tutorial, we'll integrate with Ollama for local LLMs.</p>

            <h4>Step 1: Set Up the Project Structure</h4>
            <p>Create a new Python file called <code>ollama_integration.py</code>:</p>

            <div class="code-block">
                <div class="code-header">
                    <span>ollama_integration.py</span>
                </div>
                <pre><code class="language-python">"""
Integrating with Ollama for local LLMs.
"""
from moya.agents.ollama_agent import OllamaAgent, OllamaAgentConfig
from moya.memory.in_memory_repository import InMemoryRepository
from moya.orchestrators.simple_orchestrator import SimpleOrchestrator
from moya.tools.tool_registry import ToolRegistry
from moya.tools.memory_tool import MemoryTool
from moya.registry.agent_registry import AgentRegistry
</code></pre>
            </div>

            <h4>Step 2: Set Up the Agent</h4>
            <p>Next, we'll set up the memory repository and tool registry. We will then create an Ollama Agent. This agent uses local LLMs hosted on your device.</p>

            <div class="code-block">
                <div class="code-header">
                    <span>ollama_integration.py (continued)</span>
                </div>
                <pre><code class="language-python">def setup_agent():
    # Set up memory components
    memory_repo = InMemoryRepository()
    memory_tool = MemoryTool(memory_repository=memory_repo)
    tool_registry = ToolRegistry()
    tool_registry.register_tool(memory_tool)

    # Create Ollama agent with memory capabilities and correct configuration
    agent_config = OllamaAgentConfig(
        system_prompt="You are a helpful AI assistant. Be concise and clear.",
        model_name="llama3.1",
        temperature=0.7,
        base_url="http://localhost:11434",
        context_window=4096
    )

    agent = OllamaAgent(
        agent_name="ollama_assistant",
        description="A local AI assistant powered by Ollama with memory",
        agent_config=agent_config,
        tool_registry=tool_registry
    )

    # Verify Ollama connection with simple test request
    try:
        agent.setup()
        # Test connection using handle_message
        test_response = agent.handle_message("test connection")
        if not test_response:
            raise Exception("No response from Ollama test query")
    except Exception as e:
        print("\nError: Make sure Ollama is running and the model is downloaded:")
        print("1. Start Ollama: ollama serve")
        print("2. Pull model: ollama pull llama3.1:latest")
        sys.exit(1)

    # Set up registry and orchestrator
    agent_registry = AgentRegistry()
    agent_registry.register_agent(agent)

return agent</code></pre>
            </div>

            <h4>Step 3: Create Helper Functions</h4>
            <p>Add a function to format conversation context:</p>

            <div class="code-block">
                <div class="code-header">
                    <span>ollama_integration.py (continued)</span>
                </div>
                <pre><code class="language-python">def format_conversation_context(messages):
    """Format conversation history for context."""
    context = "\nPrevious conversation:\n"
    for msg in messages:
        sender = "User" if msg.sender == "user" else "Assistant"
        context += f"{sender}: {msg.content}\n"
    return context</code></pre>
            </div>

            <h4>Step 4: Implement the Main Function</h4>
            <p>Now, let's create the main function for our interactive chat:</p>

            <div class="code-block">
                <div class="code-header">
                    <span>ollama_integration.py (continued)</span>
                </div>
                <pre><code class="language-python">def main():
    agent = setup_agent()
    thread_id = "interactive_chat_001"

    print("Welcome to Interactive Chat! (Type 'quit' or 'exit' to end)")
    print("-" * 50)

    while True:
        user_input = input("\nYou: ").strip()

        if user_input.lower() in ['quit', 'exit']:
            print("\nGoodbye!")
            break

        logger.debug(f"User input: {user_input}")

        # Store user message
        agent.call_tool(
            tool_name="MemoryTool",
            method_name="store_message",
            thread_id=thread_id,
            sender="user",
            content=user_input
        )

        # Get conversation context
        previous_messages = agent.get_last_n_messages(thread_id, n=5)

        if previous_messages:
            context = format_conversation_context(previous_messages)
            enhanced_input = f"{context}\nCurrent user message: {user_input}"
        else:
            enhanced_input = user_input

        logger.debug(f"Enhanced input: {enhanced_input}")

        try:
            print("\nAssistant: ", end="", flush=True)

            response = ""
            try:
                # Use enhanced_input instead of user_input for context
                for chunk in agent.handle_message_stream(enhanced_input):
                    if chunk:
                        print(chunk, end="", flush=True)
                        response += chunk
            except Exception as e:
                logger.error(f"Streaming error: {e}")
                # Fallback to non-streaming with enhanced input
                response = agent.handle_message(enhanced_input)
                if response:
                    print(response)

            print()

            if not response or response.startswith("[OllamaAgent error"):
                print("\nError: No response received. Please try again.")
                continue

            # Store the assistant's response
            agent.call_tool(
                tool_name="MemoryTool",
                method_name="store_message",
                thread_id=thread_id,
                sender="assistant",
                content=response
            )

        except Exception as e:
            logger.error(f"Error during chat: {e}")
            print("\nAn error occurred. Please try again.")
            continue


if __name__ == "__main__":
    main()
                    </code></pre>
            </div>

            <h4>Step 5: Run the Agent</h4>
            <p>Make sure that Ollama service is running and Llama3.1 model is downloaded, then run the script:</p>

            <div class="code-block">
                <div class="code-header">
                    <span>Terminal</span>
                </div>
                <pre><code class="language-bash">ollama serve
ollama pull llama3.1:latest
python ollama_integration.py</code></pre>
            </div>

            <p>You should now be able to chat with your agent, and it will remember the conversation history!</p>

            <div class="note">
                <strong>Note:</strong> This tutorial is similar to the <code>quick_start_ollama.py</code> example
                included in the Moya repository. You can run that example directly with
                <code>python examples/quick_start_ollama.py</code>.
            </div>

        </main>
    </div>
</body>

</html>